{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is aimed to documents the steps taken to build and evaluate a predictive model for inventory planning.<br>\n",
    "The goal is to predict the quantity of items sold based on historical sales data and various features.<br>\n",
    "The model will help in making informed decisions for inventory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation, Feature Engineering, and Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load the historical sales data. This involves:\n",
    "\n",
    "1. **Loading the Data**: Fetching the data and building clean db in the DuckDB database.\n",
    "3. **Feature Engineering**: Creating the features to enhance the model's ability to capture patterns in the data.\n",
    "\n",
    "### Loading and Exploring Data\n",
    "- **Download Data**: Download data from GDrive.\n",
    "- **Load Data**: Put the data in a table in DuckDB.\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "We create new features from existing ones to improve the performance of the model.:\n",
    "\n",
    "- **Temporal Features**: *is_weekend*, *day_of_week*, *day_of_year*, and *week_of_year* to capture temporal patterns.\n",
    "- **Lag Features**: *quantity_sold_lag_1* and *quantity_sold_lag_7* to incorporate historical sales data.\n",
    "- **Rolling Mean**: *rolling_mean_7* to smooth out fluctuations and capture trends.\n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "To ensure that all SKUs are represented in the training set and to maintain the temporal order within each SKU, we perform the following steps:\n",
    "\n",
    "1. **Data Retrieval**: Fetch the data from the DuckDB database.\n",
    "2. **SKU-Specific Data**: For each unique SKU, sort the data by date (*dt_submitted*) to maintain temporal order.\n",
    "3. **Train-Test Split per SKU**: \n",
    "   - For SKUs with more than one record, split the data into training and testing sets using an 80-20 split.\n",
    "   - For SKUs with only one record, include the record in the training set.\n",
    "4. **Combine Splits**: Aggregate the splits from all SKUs into combined training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>dt_submitted</th>\n",
       "      <th>quantity_sold</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>quantity_sold_lag_1</th>\n",
       "      <th>quantity_sold_lag_7</th>\n",
       "      <th>rolling_mean_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1001484580476486315</td>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>330</td>\n",
       "      <td>47</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1001484580476486315</td>\n",
       "      <td>2023-03-29</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sku dt_submitted  quantity_sold  day  year  month  \\\n",
       "0  -1001484580476486315   2022-11-26              1   26  2022     11   \n",
       "1  -1001484580476486315   2023-03-29              1   29  2023      3   \n",
       "\n",
       "   day_of_week  day_of_year  week_of_year  is_weekend  quantity_sold_lag_1  \\\n",
       "0            6          330            47        True                    0   \n",
       "1            3           88            13       False                    1   \n",
       "\n",
       "   quantity_sold_lag_7  rolling_mean_7  \n",
       "0                    0             1.0  \n",
       "1                    0             1.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tasks.prepare\n",
    "\n",
    "cache_dir = \"../.cache/\"\n",
    "con, df_train, df_test = tasks.prepare.get_data(cache_dir=cache_dir)\n",
    "con.query(query=\"SELECT * FROM features LIMIT 2\").fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building & Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our model by integrating a preprocessing pipeline, hyperparameter tuning and using a time-series aware cross-validation strategy:\n",
    "\n",
    "### Pipeline and Preprocessing\n",
    "\n",
    "- **Pipeline**: We use a *Pipeline* to streamline the preprocessing and modeling steps.\n",
    "- **Preprocessor**: A *ColumnTransformer* is used to handle different preprocessing steps for numerical and categorical features. Numerical features are passed through as is (*passthrough*), while categorical features (*sku*) are one-hot encoded.\n",
    "\n",
    "### Hyperparameter Tuning with RandomizedSearchCV\n",
    "\n",
    "- **RandomizedSearchCV**: Instead of an exhaustive grid search, we use *RandomizedSearchCV* to randomly sample a specified number of hyperparameter combinations (n_iter=50) from the defined parameter distribution (*param_dist*). This is more efficient and can still cover a broad search space.\n",
    "\n",
    "- **Parameter Distribution**: We define a range of values for key hyperparameters of the *XGBRegressor*:\n",
    "  - *max_depth*: Controls the depth of each tree.\n",
    "  - *n_estimators*: The number of boosting rounds.\n",
    "  - *learning_rate*: The step size shrinkage used to prevent overfitting.\n",
    "  - *subsample*: The fraction of samples to be used for fitting the trees.\n",
    "  - *colsample_bytree*: The fraction of features to be used for each tree.\n",
    "\n",
    "### Time Series Cross-Validation\n",
    "- **TimeSeriesSplit**: We use *TimeSeriesSplit* with 5 splits to ensure that the temporal order of data is respected during cross-validation. This helps in preventing data leakage and provides a more realistic evaluation of the model's performance on future data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/badrayour.el-amri/Library/Caches/pypoetry/virtualenvs/inventory-planning-osy1kPWt-py3.11/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tasks.train\n",
    "import helpers.model\n",
    "\n",
    "y_train = df_train[[\"quantity_sold\"]]\n",
    "x_train = df_train.drop(labels=\"quantity_sold\", axis=1)\n",
    "\n",
    "grid_search = helpers.model.get_model()\n",
    "model = tasks.train.train(\n",
    "    y_train=y_train,\n",
    "    x_train=x_train,\n",
    "    grid_search=grid_search,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we evaluate its performance on the test dataset to ensure it generalizes well to unseen data:\n",
    "\n",
    "- **R-squared (RÂ²)**: Indicates the proportion of variance in the target variable that is predictable from the features.\n",
    "- **Mean Absolute Error (MAE)**: The average absolute difference between predicted and actual values.\n",
    "- **Mean Squared Error (MSE)**: The average squared difference between predicted and actual values.\n",
    "- **Root Mean Squared Error (RMSE)**: The square root of the MSE, providing an error metric in the same units as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity sold data distrib: {'count': 1026036.0, 'mean': 6.056740699156755, 'std': 26.17813760095008, 'min': 0.0, '25%': 1.0, '50%': 2.0, '75%': 5.0, 'max': 10040.0}\n",
      "Metrics: {'r2': 0.7499997019767761, 'mae': 2.6746642945269583, 'mse': 148.86490380974786, 'rmse': 12.201020605250523}\n"
     ]
    }
   ],
   "source": [
    "import tasks.evaluate\n",
    "\n",
    "y_test = df_test[[\"quantity_sold\"]]\n",
    "x_test = df_test.drop(labels=\"quantity_sold\", axis=1)\n",
    "metrics = tasks.evaluate.evaluate(\n",
    "    model=model,\n",
    "    x_test=x_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "print(\"Quantity sold data distrib:\", con.execute(\"SELECT quantity_sold FROM features\").fetch_df().describe().to_dict()[\"quantity_sold\"])\n",
    "print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **R-squared (RÂ²)**:\n",
    "   - **Value**: `0.75`\n",
    "   - **Interpretation**: 75% of the variance in the `quantity_sold` can be explained by the model. This is a strong indication that the model has captured patterns in the data.\n",
    "\n",
    "2. **Mean Absolute Error (MAE)**:\n",
    "   - **Value**: `2.67`\n",
    "   - **Interpretation**: On average, our model's predictions are off by about 2.67 units of the `quantity_sold`. Given the mean `quantity_sold` is around 6.06, this is a relatively low error, indicating good predictive accuracy.\n",
    "\n",
    "3. **Mean Squared Error (MSE)**:\n",
    "   - **Value**: `148.86`\n",
    "   - **Interpretation**: There are some larger errors, but this is expected given the variability in the data. The question of outlier processing can be asked.\n",
    "\n",
    "4. **Root Mean Squared Error (RMSE)**:\n",
    "   - **Value**: `12.20`\n",
    "   - **Interpretation**: The typical prediction error is around 12 units. This value is higher than the MAE due to the squaring of errors in the MSE, which penalizes larger errors more heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Metrics to Provide Prediction Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a range for the predicted quantity sold, we can use the Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE).<br>\n",
    "These metrics help us quantify the uncertainty in our predictions.\n",
    "\n",
    "#### Prediction Intervals\n",
    "\n",
    "Prediction intervals give us a range within which the actual quantity sold is likely to fall.<br>\n",
    "This range is calculated by adding and subtracting the error metric (MAE or RMSE) from the predicted value.\n",
    "\n",
    "#### MAE\n",
    "- **Mean Absolute Error (MAE)**: It provides a straightforward way to calculate the prediction interval as it directly reflects the average error.\n",
    "- **Calculation**: \n",
    "  - Lower Bound: `Prediction - MAE`\n",
    "  - Upper Bound: `Prediction + MAE`\n",
    "\n",
    "#### RMSE\n",
    "- **Root Mean Squared Error (RMSE)**: The RMSE gives more weight to larger errors due to the squaring of differences before taking the average. It reflects the typical size of the prediction error in the same units as the target variable.\n",
    "- **Calculation**: \n",
    "  - Lower Bound: `Prediction - RMSE`\n",
    "  - Upper Bound: `Prediction + RMSE`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inventory-planning-osy1kPWt-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
